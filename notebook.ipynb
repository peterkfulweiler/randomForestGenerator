{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 1 , validation accuracy: 0.9760416666666666 , validation mistakes: 45.2\n",
      "num_iterations: 10 , validation accuracy: 0.9697916666666666 , validation mistakes: 21.02\n",
      "num_iterations: 100 , validation accuracy: 0.9895833333333333 , validation mistakes: 12.044\n",
      "num_iterations: 200 , validation accuracy: 0.990625 , validation mistakes: 10.401\n",
      "num_iterations: 1000 , validation accuracy: 0.9864583333333332 , validation mistakes: 7.978\n",
      "Best_Num_Iterations:  200 Best Learning Rate:  1\n",
      "train accuracy: 0.990625 , test accuracy: 0.9733009708737864\n",
      "############## Predictions ################# \n",
      "Number of Models:  1 Number of features:  1 Accuracy:  0.3713592233009709\n",
      "############## Predictions ################# \n",
      "Number of Models:  1 Number of features:  2 Accuracy:  0.8495145631067961\n",
      "############## Predictions ################# \n",
      "Number of Models:  1 Number of features:  3 Accuracy:  0.5558252427184466\n",
      "############## Predictions ################# \n",
      "Number of Models:  1 Number of features:  4 Accuracy:  0.9854368932038835\n",
      "############## Predictions ################# \n",
      "Number of Models:  1 Number of features:  5 Accuracy:  0.9878640776699029\n",
      "############## Predictions ################# \n",
      "Number of Models:  2 Number of features:  1 Accuracy:  0.49271844660194175\n",
      "############## Predictions ################# \n",
      "Number of Models:  2 Number of features:  2 Accuracy:  0.8325242718446602\n",
      "############## Predictions ################# \n",
      "Number of Models:  2 Number of features:  3 Accuracy:  0.7354368932038835\n",
      "############## Predictions ################# \n",
      "Number of Models:  2 Number of features:  4 Accuracy:  0.9854368932038835\n",
      "############## Predictions ################# \n",
      "Number of Models:  2 Number of features:  5 Accuracy:  0.9854368932038835\n",
      "############## Predictions ################# \n",
      "Number of Models:  3 Number of features:  1 Accuracy:  0.837378640776699\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g8/946vl3l15qv6mp2wgbzl6h0c0000gn/T/ipykernel_23481/1425752111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumaverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmultiplemodelaccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monemodelaccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_num_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestnumiterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hyper_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_straps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmultiple_model_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiplemodelaccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0monemodel_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monemodelaccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/checkpoint1.py\u001b[0m in \u001b[0;36mget_hyper_parameters\u001b[0;34m(train_df, test_df, features, label, num_iterations, learning_rate, num_models, num_straps, num_features)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             forest = perceptron_forrest(\n\u001b[0m\u001b[1;32m    165\u001b[0m                 train_df, features, label, i, num_straps, j, bestnumiterations, learning_rate)\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# print(\"############## Forest ################# \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/checkpoint1.py\u001b[0m in \u001b[0;36mperceptron_forrest\u001b[0;34m(train_df, features, label, n_submodels, n_bootstrap, n_features, num_iterations, learning_rate)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# Get Weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m#print(X, \": X\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         percept_w = perceptron.perceptron(\n\u001b[0m\u001b[1;32m    145\u001b[0m             X.transpose(), Y, learning_rate, num_iterations)\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# return weights and random_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/perceptron.py\u001b[0m in \u001b[0;36mperceptron\u001b[0;34m(X, y, learning_rate, num_iterations)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Check to see if prediction is correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0msign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# If not equal adjust weight num_mistakes +1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/perceptron.py\u001b[0m in \u001b[0;36msgn\u001b[0;34m(prod)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msgn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mprod\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Helper libraries\n",
    "import perceptron\n",
    "import decisiontree\n",
    "import util\n",
    "import checkpoint1\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv('data_banknote_authentication.csv')\n",
    "features = {'variance', 'skewness', 'curtosis', 'entropy'}\n",
    "target = 'class'\n",
    "filename = 'data_banknote_authentication.csv'\n",
    "df = util.load_data(filename)\n",
    "\n",
    "df['bias'] = 1\n",
    "features.add('bias')\n",
    "\n",
    "# Convert 0 labels to -1\n",
    "df['class'] = df['class'].apply(lambda x: 1 if x > 0 else -1)\n",
    "\n",
    "    # Split data into training and test\n",
    "train_proportion = 0.70\n",
    "train_df, test_df = util.split_data(df, train_proportion)\n",
    "\n",
    "num_models = 5\n",
    "num_straps = len(df) // 2\n",
    "num_features = df.shape[1]\n",
    "num_iterations = [1,10, 100, 200, 1000]\n",
    "lr = 1\n",
    "multiple_model_sum = []\n",
    "onemodel_sum = []\n",
    "best_models_sum = []\n",
    "best_num_features_sum = []\n",
    "best_iterations_sum = []\n",
    "numaverage = 5\n",
    "\n",
    "\n",
    "for i in range(numaverage):\n",
    "    multiplemodelaccuracy, onemodelaccuracy, best_model, best_num_feature, bestnumiterations = checkpoint1.get_hyper_parameters(train_df, test_df, features, target, num_iterations, lr, num_models, num_straps, num_features)\n",
    "    multiple_model_sum.append(multiplemodelaccuracy)\n",
    "    onemodel_sum.append(onemodelaccuracy)\n",
    "    best_models_sum.append(best_model)\n",
    "    best_num_features_sum.append(best_num_feature)\n",
    "    best_iterations_sum.append(bestnumiterations)\n",
    "\n",
    "multiple_model_sum = sum(multiple_model_sum) / numaverage\n",
    "onemodel_sum = sum(onemodel_sum) / numaverage\n",
    "best_models_sum = round(sum(best_models_sum) / numaverage)\n",
    "best_num_features_sum = round(sum(best_num_features_sum) / numaverage)\n",
    "best_iterations_sum = round(sum(best_iterations_sum) / numaverage)\n",
    "\n",
    "print(\"############# FINAL HYPERPARAMETERS ##################\")\n",
    "print(\"Average Training Accuracy for one perceptron: \", onemodel_sum)\n",
    "print(\"Average Traingin Accuracy for best hyperparameters of Perceptron Forest: \", multiple_model_sum)\n",
    "print(\"Average number of Models: \", best_models_sum)\n",
    "print(\"Average number of features: \", best_num_features_sum)\n",
    "print(\"Average best num of iterations: \", best_iterations_sum)\n",
    "#print(percept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g8/946vl3l15qv6mp2wgbzl6h0c0000gn/T/ipykernel_22278/2442151102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumaverage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmultiplemodelaccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monemodelaccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_num_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbestnumiterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hyper_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_straps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmultiple_model_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiplemodelaccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0monemodel_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monemodelaccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/checkpoint1.py\u001b[0m in \u001b[0;36mget_hyper_parameters\u001b[0;34m(train_df, test_df, features, label, num_iterations, learning_rate, num_models, num_straps, num_features)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0malso\u001b[0m \u001b[0mgets\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0mon\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m     trainaccuracy, testaccuracy, bestnumiterations, best_lr = perceptron.test_perceptron(\n\u001b[0m\u001b[1;32m    158\u001b[0m         train_df, test_df, features, label, num_iterations, learning_rate)\n\u001b[1;32m    159\u001b[0m     \u001b[0mbestaccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Random Forrest Generator/randomForestGenerator/perceptron.py\u001b[0m in \u001b[0;36mtest_perceptron\u001b[0;34m(train_df, test_df, features, label, num_iterations, learning_rate)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         validation_accuracy, validation_mistakes = perceptron_cross_validation(\n\u001b[0;32m--> 223\u001b[0;31m             train_df, num_folds, features, label, learning_rate, k)\n\u001b[0m\u001b[1;32m    224\u001b[0m         print('num_iterations:', k, ', validation accuracy:',\n\u001b[1;32m    225\u001b[0m               validation_accuracy, ', validation mistakes:', validation_mistakes)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Helper libraries\n",
    "import perceptron\n",
    "import decisiontree\n",
    "import util\n",
    "import checkpoint1\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv('sonar.csv')\n",
    "features = {'attribute_1','attribute_2','attribute_3','attribute_4','attribute_5','attribute_6','attribute_7','attribute_8','attribute_9','attribute_10','attribute_11','attribute_12','attribute_13','attribute_14','attribute_15','attribute_16','attribute_17','attribute_18','attribute_19','attribute_20','attribute_21','attribute_22','attribute_23','attribute_24','attribute_25','attribute_26','attribute_27','attribute_28','attribute_29','attribute_30','attribute_31','attribute_32','attribute_33','attribute_34','attribute_35','attribute_36','attribute_37','attribute_38','attribute_39','attribute_40','attribute_41','attribute_42','attribute_43','attribute_44','attribute_45','attribute_46','attribute_47','attribute_48','attribute_49','attribute_50','attribute_51','attribute_52','attribute_53','attribute_54','attribute_55','attribute_56','attribute_57','attribute_58','attribute_59','attribute_60'}\n",
    "target = 'Class'\n",
    "filename = 'sonar.csv'\n",
    "df = util.load_data(filename)\n",
    "\n",
    "df['bias'] = 1\n",
    "features.add('bias')\n",
    "\n",
    "# Convert 0 labels to -1\n",
    "df['Class'].replace(['Rock', 'Mine'], [-1, 1], inplace=True)\n",
    "#print(df)\n",
    "\n",
    "    # Split data into training and test\n",
    "train_proportion = 0.70\n",
    "train_df, test_df = util.split_data(df, train_proportion)\n",
    "num_models = 5\n",
    "num_straps = len(df) // 2\n",
    "num_features = df.shape[1]\n",
    "num_iterations = [1,10, 100, 200, 1000]\n",
    "lr = 1\n",
    "multiple_model_sum = []\n",
    "onemodel_sum = []\n",
    "best_models_sum = []\n",
    "best_num_features_sum = []\n",
    "best_iterations_sum = []\n",
    "numaverage = 5\n",
    "\n",
    "\n",
    "for i in range(numaverage):\n",
    "    multiplemodelaccuracy, onemodelaccuracy, best_model, best_num_feature, bestnumiterations = checkpoint1.get_hyper_parameters(train_df, test_df, features, target, num_iterations, lr, num_models, num_straps, num_features)\n",
    "    multiple_model_sum.append(multiplemodelaccuracy)\n",
    "    onemodel_sum.append(onemodelaccuracy)\n",
    "    best_models_sum.append(best_model)\n",
    "    best_num_features_sum.append(best_num_feature)\n",
    "    best_iterations_sum.append(bestnumiterations)\n",
    "\n",
    "multiple_model_sum = sum(multiple_model_sum) / numaverage\n",
    "onemodel_sum = sum(onemodel_sum) / numaverage\n",
    "best_models_sum = round(sum(best_models_sum) / numaverage)\n",
    "best_num_features_sum = round(sum(best_num_features_sum) / numaverage)\n",
    "best_iterations_sum = round(sum(best_iterations_sum) / numaverage)\n",
    "\n",
    "print(\"############# FINAL HYPERPARAMETERS ##################\")\n",
    "print(\"Average Training Accuracy for one perceptron: \", onemodel_sum)\n",
    "print(\"Average number of Models: \", best_models_sum)\n",
    "print(\"Average number of features: \", best_num_features_sum)\n",
    "print(\"Average best num of iterations: \", best_iterations_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iterations: 1 , validation accuracy: 0.975 , validation mistakes: 53.2\n",
      "learning rate:  1\n",
      "num_iterations: 10 , validation accuracy: 0.984375 , validation mistakes: 23.52\n",
      "learning rate:  1\n",
      "num_iterations: 20 , validation accuracy: 0.9875 , validation mistakes: 18.93\n",
      "learning rate:  1\n",
      "num_iterations: 50 , validation accuracy: 0.98125 , validation mistakes: 14.276\n",
      "learning rate:  1\n",
      "num_iterations: 100 , validation accuracy: 0.975 , validation mistakes: 12.251999999999999\n",
      "learning rate:  1\n",
      "Best_Num_Iterations:  20\n",
      "train accuracy: 0.9854166666666667 , test accuracy: 0.9951456310679612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Helper libraries\n",
    "import perceptron\n",
    "import decisiontree\n",
    "import util\n",
    "import checkpoint1\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.read_csv('data_banknote_authentication.csv')\n",
    "features = {'variance', 'skewness', 'curtosis', 'entropy'}\n",
    "target = 'class'\n",
    "filename = 'data_banknote_authentication.csv'\n",
    "df = util.load_data(filename)\n",
    "\n",
    "df['bias'] = 1\n",
    "features.add('bias')\n",
    "\n",
    "# Convert 0 labels to -1\n",
    "df['class'] = df['class'].apply(lambda x: 1 if x > 0 else -1)\n",
    "\n",
    "    # Split data into training and test\n",
    "train_proportion = 0.70\n",
    "train_df, test_df = util.split_data(df, train_proportion)\n",
    "\n",
    "num_models = 10\n",
    "num_straps = len(df) // 2\n",
    "num_features = df.shape[1]\n",
    "num_iterations = [1,10,20,50,100]\n",
    "lr = 1\n",
    "\n",
    "trainaccuracy, testaccuracy, bestnumiterations = perceptron.test_perceptron(\n",
    "        train_df, test_df, features, target, num_iterations, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f80b4658544bbc031aef330f6b918b3c2a5b06e50082aae5f6097f0c1fbe1a41"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}