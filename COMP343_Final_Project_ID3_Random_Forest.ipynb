{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "m0iLLIeO37fG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Python modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import preprocessing\n",
        "import sys\n",
        "\n",
        "random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "dCBJlZlpUyTB"
      },
      "outputs": [],
      "source": [
        "######################### Data helper functions #############################\n",
        "def load_data(filename):\n",
        "    ''' Returns a dataframe (df) containing the data in filename. You should\n",
        "        specify the full path plus the name of the file in filename, relative\n",
        "        to where you are running code\n",
        "    '''\n",
        "    df = pd.read_csv(filename)\n",
        "    return df\n",
        "\n",
        "def sigmoid(z):\n",
        "    ''' Sigmoid function '''\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_grad(z):\n",
        "    ''' Derivative of sigmoid function '''\n",
        "    sig_z = sigmoid(z)\n",
        "    return sig_z * (1-sig_z)\n",
        "\n",
        "def split_data(df, train_proportion):\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * train_proportion: proportion of data in df that will be used for\n",
        "                training. 1-train_proportion is proportion of data to be used\n",
        "                for testing\n",
        "        Output\n",
        "            * train_df: dataframe containing training data\n",
        "            * test_df: dataframe containing testing data\n",
        "    '''\n",
        "    # Make sure there are row numbers\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Reorder examples and split data according to train proportion\n",
        "    train = df.sample(frac=train_proportion, axis=0)\n",
        "    test = df.drop(index=train.index)\n",
        "    return train, test\n",
        "\n",
        "def divide_k_folds(df, num_folds):\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * num_folds: number of folds\n",
        "        Output\n",
        "            * folds: lists of folds, each fold is subset of df dataframe\n",
        "    '''\n",
        "    folds = []\n",
        "    for subset in np.array_split(df, num_folds):\n",
        "        folds.append(subset)\n",
        "\n",
        "    return folds\n",
        "\n",
        "def to_numpy(df):\n",
        "    a = df.to_numpy()\n",
        "    return a.T\n",
        "\n",
        "def get_X_y_data(df, features, target):\n",
        "    ''' Split dataframe into X and y numpy arrays '''\n",
        "    X_df = df.loc[:, df.columns != target]\n",
        "    Y_df = df[target]\n",
        "    # X = to_numpy(X_df)\n",
        "    # Y = to_numpy(Y_df)\n",
        "    return X_df, Y_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2oEcyrT22p1Q"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    ''' Class for nodes in decision tree '''\n",
        "    def __init__(self, feature_name='', feature_value=None):\n",
        "        self.feature_name = feature_name # head if dummy head node, otherwise is feature or label value\n",
        "        self.feature_value = feature_value # head if dummy head node, otherwise is feature or label value\n",
        "        self.children = None # list of nodes\n",
        "\n",
        "def print_tree(node, i):\n",
        "    ''' Helper function that prints decision tree.\n",
        "        Inputs:\n",
        "            * node: root node of decision tree to print\n",
        "            * i: number of tabs to offset each layer of decision tree\n",
        "                 when printing.\n",
        "    '''\n",
        "    print(\"hi\")\n",
        "    if node.children is None:\n",
        "        print(\"two\")\n",
        "        tabs = i * '\\t'\n",
        "        print(tabs + str(node.feature_value))\n",
        "    else:\n",
        "        print(\"one\")\n",
        "        tabs = i * '\\t'\n",
        "        print(tabs + str(node.feature_name) + ':' + str(node.feature_value))\n",
        "        for child in node.children:\n",
        "            print_tree(child, i + 1)\n",
        "\n",
        "\"\"\"\n",
        "HELPER FUNCTIONS\n",
        "\"\"\"\n",
        "\n",
        "def get_children_from_fvals(df, f):\n",
        "    ''' Inputs:\n",
        "            * df: dataframe containing data\n",
        "            * f: name of current feature being considered\n",
        "        Output:\n",
        "            * List of child nodes from feature values for f\n",
        "    '''\n",
        "    fvals = df[f].unique()\n",
        "    return [Node(f, v) for v in fvals]\n",
        "\n",
        "def get_df_num_rows(df):\n",
        "    ''' Returns number of rows in dataframe '''\n",
        "    return len(df)\n",
        "\n",
        "def get_df_subset(df, f, fval):\n",
        "    ''' Inputs:\n",
        "            * df: dataframe containing data\n",
        "            * f: name of current feature being considered\n",
        "            * fval: one value that f can take on\n",
        "        Output:\n",
        "            * Dataframe comprising rows of df for which f's value is fval\n",
        "    '''\n",
        "    df_fval = df[ df[f] == fval ]\n",
        "    num_fval = get_df_num_rows(df_fval)\n",
        "    return df_fval, num_fval\n",
        "\n",
        "def get_df_num_labels(df, label):\n",
        "    ''' Inputs:\n",
        "            * df: dataframe containing data\n",
        "            * label: column name in df to use as label\n",
        "        Output:\n",
        "            * Dictionary, where keys are label value in df_val and values the\n",
        "            number of rows in df with that label\n",
        "    '''\n",
        "    num_label = df[label].value_counts().to_dict()\n",
        "    return num_label\n",
        "\n",
        "def information_gain(df, features, label):\n",
        "    ''' Inputs:\n",
        "            * df: dataframe containing data\n",
        "            * features: current features to consider\n",
        "            * label: column name in df to use as label\n",
        "        Output:\n",
        "            * Feature name on which to split, i.e., feature with the maximum\n",
        "              information gain\n",
        "    '''\n",
        "\n",
        "    # How many rows in dataframe are there? Each row is an instance\n",
        "    num_instances = get_df_num_rows(df)\n",
        "    min_entropy = None\n",
        "    split_on = None\n",
        "\n",
        "    # Iterate through each feature/category to determine\n",
        "    # which gives maximum information gain\n",
        "    for f in features:\n",
        "        sum_entropy = 0\n",
        "\n",
        "        # Get all values of the current feature to split the dataset\n",
        "        for fval in df[f].unique():\n",
        "\n",
        "            # Get rows of dataframe for which f's value is fval\n",
        "            df_fval, num_fval = get_df_subset(df, f, fval)\n",
        "\n",
        "            # Get counts for each possible label value in df_val\n",
        "            num_label_fval = get_df_num_labels(df_fval, label)\n",
        "\n",
        "            # Calculate entropy\n",
        "            entropy = 0\n",
        "            for _, num_label in num_label_fval.items():\n",
        "                prob = num_label/num_fval\n",
        "                entropy += - (prob)*np.log2(prob)\n",
        "            sum_entropy += (num_fval/num_instances) * entropy\n",
        "\n",
        "        # Get feature with minimum entropy sum\n",
        "        # because this feaure has the maximum info gain\n",
        "        if min_entropy is None or sum_entropy < min_entropy:\n",
        "            min_entropy = sum_entropy\n",
        "            split_on = f\n",
        "\n",
        "    return split_on\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "ID3 Algorithm\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def get_ID3_num_correct(df, parent, label):\n",
        "    ''' Inputs:\n",
        "            * df: dataframe containing data\n",
        "            * node: root node in decision tree\n",
        "        Output\n",
        "            * num_correct: number of correct predictions made on data in df by tree\n",
        "    '''\n",
        "    if parent.children is None:\n",
        "        return len(df)\n",
        "\n",
        "    num_correct = 0\n",
        "    for child in parent.children:\n",
        "        subset = df.copy()\n",
        "        subset = subset[subset[child.feature_name] == child.feature_value]\n",
        "        num_correct += get_ID3_num_correct(subset, child, label)\n",
        "    return num_correct\n",
        "\n",
        "def get_ID3_accuracy(df, dtree, features, label):\n",
        "    num_correct = get_ID3_num_correct(df, dtree, label)\n",
        "    return num_correct / len(df)\n",
        "\n",
        "root_node = Node('root')\n",
        "\n",
        "def ID3_build_tree(df, features, label, parent, max_depth):\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * features: current features to consider\n",
        "            * label: column name in df to use as label\n",
        "            * parent: of type Node\n",
        "        Output\n",
        "            * a decision tree\n",
        "    '''\n",
        "\n",
        "    #####\n",
        "    # Todo: update this function to stop once max depth hit\n",
        "    #####\n",
        "    depth_count = max_depth\n",
        "    # if all features have been used, return most popular label\n",
        "    # if there is only one label, also return\n",
        "    if len(features) == 0 or len( df[label].unique() ) == 1 or depth_count == 0:\n",
        "        # Get most frequent label using mode\n",
        "        leaf = df[label].mode()[0]\n",
        "\n",
        "        # Set children to be leaf node containing most frequent label\n",
        "        parent.children = [Node(label, leaf)]\n",
        "\n",
        "    # Otherwise, continue to recurse down the tree\n",
        "    else:\n",
        "\n",
        "        # Get feature with the most info gain\n",
        "        feature_to_split_on = information_gain(df, features, label)\n",
        "\n",
        "        # Values from that feature become the children of the previous node\n",
        "        parent.children = get_children_from_fvals(df, feature_to_split_on)\n",
        "\n",
        "        # Remove the feature we just split on so we don't try to split on it again\n",
        "        new_features = features.copy()\n",
        "        new_features.remove(feature_to_split_on)\n",
        "\n",
        "        # Recursively call build_tree on each child node\n",
        "        for child in parent.children:\n",
        "            ID3_build_tree(df[ df[feature_to_split_on] == child.feature_value ],\n",
        "                    new_features, label, child,max_depth-1)\n",
        "        depth_count -= 1\n",
        "\n",
        "def ID3_decision_tree(df, features, label, max_depth=5, random_subspace = None):\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * features: list of current features to consider\n",
        "            * label: column name in df to use as label\n",
        "            * max_depth: max depth of tree\n",
        "            * random_subspace: number of random features to be chosen\n",
        "        Output\n",
        "            * dtree: root node of trained decision tree\n",
        "    '''\n",
        "    # get the number of columns of the dataframe\n",
        "    _, n_col = df.shape\n",
        "    # get the indeces of the columns excluding target column\n",
        "    n_col_indeces = list(range(n_col-1)) \n",
        "    # check if random\n",
        "    if random_subspace != None:\n",
        "      n_col_indeces = random.sample(population = n_col_indeces, k = random_subspace)\n",
        "    # initialize empty list of random features\n",
        "    random_features = []\n",
        "    # append randomly chosen features to the random_features list\n",
        "    for index in n_col_indeces:\n",
        "      random_features.append(features[index])\n",
        "    # initialize root node\n",
        "    dtree = Node('root', '')\n",
        "    # build tree using random features\n",
        "    ID3_build_tree(df, random_features, label, dtree, max_depth)\n",
        "    return dtree\n",
        "\n",
        "def ID3_regular(df, features, label, max_depth): #ID3 decision tree algo from class\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * features: current features to consider\n",
        "            * label: column name in df to use as label\n",
        "        Output\n",
        "            * dtree: root node of trained decision tree\n",
        "    '''\n",
        "    dtree = Node('root', '')\n",
        "    ID3_build_tree(df, features, label, dtree, max_depth)\n",
        "    #print_tree(dtree, 0)\n",
        "    return dtree\n",
        "\n",
        "def ID3_cross_validation(df, num_folds, features, label, tree_depths = 100): #cross validation from class\n",
        "    ''' Inputs\n",
        "            * df: dataframe containing data\n",
        "            * num_folds: number of folds (k) for cross-validation\n",
        "            * features: featurs to use (list of names)\n",
        "            * label: column name in df to use as label\n",
        "        Output\n",
        "            * train_accuracy: dataframe containing training data\n",
        "            * test_accuracy: dataframe containing testing data\n",
        "    '''\n",
        "\n",
        "    #####\n",
        "    # Todo: Implement cross-validation and train tree for different depths\n",
        "    ####\n",
        "\n",
        "    train, test = split_data(df,0.70)\n",
        "\n",
        "    folds = divide_k_folds(train, num_folds)\n",
        "\n",
        "    best_accuracy = -9999999\n",
        "    best_depth = None\n",
        "\n",
        "    depth_acc = []\n",
        "\n",
        "    for depth in tree_depths:\n",
        "\n",
        "        accuracies= []\n",
        "        for i in range(0,num_folds):\n",
        "            #removes validation subset from df\n",
        "            validation = folds[i]\n",
        "            exc_val = train.merge(validation, how='left', indicator=True)\n",
        "            exc_val = exc_val[exc_val['_merge'] == 'left_only']\n",
        "\n",
        "            # model is trained with remaining\n",
        "            dtree = ID3_regular(exc_val, features, label, depth)\n",
        "\n",
        "            # gets accuracy of the validation set\n",
        "            accuracy = get_ID3_accuracy(validation, dtree, features, label)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        mean_accuracy = sum(accuracies)/len(accuracies)\n",
        "        if mean_accuracy > best_accuracy:\n",
        "           best_accuracy = mean_accuracy\n",
        "           best_depth = depth\n",
        "\n",
        "        depth_acc.append([depth, mean_accuracy])\n",
        "\n",
        "        # print('depth ' + str(depth) + ' has ' + str(mean_accuracy) + \" accuracy\")\n",
        "        # print('best depth so far is ' + str(best_depth) + \" with accuracy of \" + str(best_accuracy))\n",
        "\n",
        "    depth_accuracy_df = pd.DataFrame(depth_acc, columns = ['Depth', 'Mean Accuracy'])\n",
        "\n",
        "    dtree = ID3_decision_tree(train, features, label, best_depth)\n",
        "    test_accuracy = get_ID3_accuracy(test, dtree, features, label)\n",
        "\n",
        "    return best_depth, depth_accuracy_df, test_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "IMGZqIf48-UR"
      },
      "outputs": [],
      "source": [
        "# CODE FOR FINDING ONE PREDICTION\n",
        "def ID3_decision_tree_prediction(tree, example):\n",
        "  if tree.children is None:\n",
        "    return tree.feature_value\n",
        "  else:\n",
        "    for child in tree.children:\n",
        "      fval = example[child.feature_name]\n",
        "      if child.feature_value == fval:\n",
        "        return ID3_decision_tree_prediction(child,example)\n",
        "      elif child.children is None:\n",
        "        return ID3_decision_tree_prediction(child,example)\n",
        "\n",
        "# CODE FOR FINING ALL PREDICTIONS FOR ONE TREE\n",
        "def ID3_decision_tree_all(tree, df):\n",
        "  # get number of examples\n",
        "  num_examples = df.shape[0]\n",
        "  # initialize empty list of predictions\n",
        "  predictions = []\n",
        "  # loop through all of the examples\n",
        "  for example in range(num_examples):\n",
        "    # get prediction for one example\n",
        "    pred = ID3_decision_tree_prediction(tree,df.iloc[example])\n",
        "    # append to list of predictions\n",
        "    predictions.append(pred)\n",
        "  return predictions\n",
        "\n",
        "def get_random_forest_predictions(df, forest):\n",
        "  # initalize dictionary for predictions\n",
        "  random_forest_predictions = {}\n",
        "  # loop through every forest\n",
        "  for i in range(len(forest)):\n",
        "    # make the column names\n",
        "    col_name = \"dtree_{}\".format(i)\n",
        "    # get predictions for each example for one decision tree\n",
        "    pred = ID3_decision_tree_all(forest[i],df)\n",
        "    # add the prediction to the dictionary\n",
        "    random_forest_predictions[col_name] = pred\n",
        "    # make dictionary a dataframe\n",
        "    pred_df = pd.DataFrame(random_forest_predictions)\n",
        "    # get the mode of each row\n",
        "    prediction = pred_df.mode(axis=1,dropna=True)[0]\n",
        "    # replace nan values with 0 (this is a precaution because it caused serious problems)\n",
        "    prediction = prediction.replace(np.nan,0)\n",
        "  return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Fy3Gtco5Uidh"
      },
      "outputs": [],
      "source": [
        "def bootstrapping(df, num_df):\n",
        "    \"\"\"\n",
        "    gets random samples with replacement and from a dataframe and creates a \n",
        "    new sub dataframe\n",
        "    \"\"\"\n",
        "    for i in range(num_df):\n",
        "        indices = np.random.randint(0, len(df), size=num_df)\n",
        "        df_bootstrapped = df.iloc[indices]\n",
        "    return df_bootstrapped\n",
        "\n",
        "def random_forest_algorithm(train_df, features, label, n_trees, n_bootstrap, n_features, max_depth):\n",
        "    \"\"\" \n",
        "    Takes a train data frame and returns a list of trees generated from bootstrapped data.\n",
        "    The length of the list depends on the number of classifiers (n_trees)\n",
        "    Outputs:\n",
        "    * a list of decision trees (forest)\n",
        "    \"\"\"\n",
        "    forest = []\n",
        "    for tree in range(n_trees):\n",
        "        df_bootstrapped = bootstrapping(train_df, n_bootstrap)\n",
        "        tree_id3 = ID3_decision_tree(df_bootstrapped, features, label, max_depth,n_features)\n",
        "        forest.append(tree_id3)\n",
        "    return forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "eGeRF3ABj9HT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_best_hyperparameters(train_data, test_data, features, label, n_trees, n_bootstrap, n_features, max_depth):\n",
        " \n",
        "  best_depth = None\n",
        "  best_accuracy = -999999999\n",
        "\n",
        "  for depth in max_depth:\n",
        "      # get the forest\n",
        "      forest = random_forest_algorithm(train_data, features, label, n_trees, n_bootstrap, n_features, depth)\n",
        "      # get final predictions for train\n",
        "      predictions_train = get_random_forest_predictions(train_data, forest)\n",
        "      # get final predictions for test\n",
        "      predictions_test = get_random_forest_predictions(test_data, forest)\n",
        "      #get accuracy \n",
        "      accuracy_train = accuracy_score(train_data[label],predictions_train)\n",
        "      accuracy_test = accuracy_score(test_data[label],predictions_test)\n",
        "      print('max_depth:', depth, ', \\t train_accuracy: ', accuracy_train,  ', \\t test_accuracy:', accuracy_test)\n",
        "\n",
        "      if accuracy_train >= best_accuracy:\n",
        "          best_accuracy = accuracy_train\n",
        "          best_depth = depth\n",
        "\n",
        "  return best_depth\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "1UAJW2pPbMj8",
        "outputId": "21d81ce2-b4f7-48eb-dbf5-b5320b2fc617"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d5b4dbb2751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load Data and drop NAs (only 1 NA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msonar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sonar.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msonar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msonar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "################## TESTING ID3 RANDOM FOREST ON SONAR DATA SET ##################\n",
        "\n",
        "#NOTE: If keep getting error in \"get hyper parameters\", run this again\n",
        "\n",
        "# Load Data and drop NAs (only 1 NA)\n",
        "sonar = pd.read_csv(\"/content/sonar.csv\")\n",
        "sonar = sonar.dropna()\n",
        "\n",
        "###### BIN STRATEGY 1 (MEAN) ######\n",
        "\n",
        "## Data Preprocessing \n",
        "\n",
        "# get the mean of each column and turn to list\n",
        "col_mean = sonar.mean(axis=0).tolist()\n",
        "\n",
        "# initalize empty df\n",
        "sonar_bin1 = pd.DataFrame()\n",
        "\n",
        "# initilaize empty list for features\n",
        "sonar_features = []\n",
        "\n",
        "# add new labels to empty df based on their values relative to the column mean\n",
        "for i in range(len(col_mean)):\n",
        "  col_name = \"attribute_{}\".format(i+1)\n",
        "  # extract features\n",
        "  sonar_features.append(col_name)\n",
        "  sonar_bin1[col_name] = pd.cut(sonar.iloc[:,i],\n",
        "                                      bins = [0,col_mean[i],1],\n",
        "                                      labels = [\"lower\",\"higher\"])\n",
        "  \n",
        "# add target column to new df and replace\n",
        "sonar_bin1['class'] = sonar['Class']\n",
        "sonar_bin1['class'] = sonar_bin1['class'].replace(['Rock','Mine'],[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "FTcEFuB2lFXQ"
      },
      "outputs": [],
      "source": [
        "## Training and Testing Sonar Data Set with Bin Strategy 1 for ID3 Random Forest\n",
        "\n",
        "# initialize parameters\n",
        "sonar_boot = int(sonar_bin1.shape[0]*0.7)\n",
        "sonar_n_features = int(np.log2(len(sonar_features)+1)) #used fewer features due to overfitting (log2 vs. to square root)\n",
        "sonar_label = 'class'\n",
        "sonar_n_trees = 50\n",
        "\n",
        "# split dataset into train and test\n",
        "train_sonar1, test_sonar1 = split_data(sonar_bin1, 0.7)\n",
        "# omit any na's just in case\n",
        "train_sonar1 = train_sonar1.dropna()\n",
        "test_sonar1 = test_sonar1.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS14tGEiQtds",
        "outputId": "78418b84-072a-4e17-f796-0cd49a4c330c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(sonar_n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y31J3Bcjxa-e",
        "outputId": "de33be23-90cb-40ff-afdd-f2c92c50c13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth: 3 , \t train_accuracy:  0.8413793103448276 , \t test_accuracy: 0.7096774193548387\n",
            "max_depth: 4 , \t train_accuracy:  0.9172413793103448 , \t test_accuracy: 0.7741935483870968\n",
            "max_depth: 5 , \t train_accuracy:  0.9586206896551724 , \t test_accuracy: 0.8225806451612904\n",
            "The best depth: 5\n"
          ]
        }
      ],
      "source": [
        "# Get hyper parameters \n",
        "best_sb1_depth = get_best_hyperparameters(train_sonar1,test_sonar1,sonar_features,sonar_label,\n",
        "                                                    sonar_n_trees, sonar_boot, sonar_n_features,[3,4,5])\n",
        "print(\"The best depth: \" + str(best_sb1_depth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0m2roJBECJb",
        "outputId": "d97a175b-3da5-4d33-ee11-8c601c67c4e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.9227586206896552\n",
            "Average Test Accuracy 0.7387096774193548\n",
            "[0.9172413793103448, 0.9448275862068966, 0.9448275862068966, 0.8827586206896552, 0.9241379310344827]\n",
            "[0.7419354838709677, 0.7419354838709677, 0.7096774193548387, 0.7258064516129032, 0.7741935483870968]\n"
          ]
        }
      ],
      "source": [
        "# Get Average Accuracies for 5 tests for Bin Strategy 1\n",
        "\n",
        "# initialize empty lists\n",
        "train_accuracies1 = []\n",
        "test_accuracies1 = []\n",
        "# loop 5 times\n",
        "for i in range(5):\n",
        "  # generate forest\n",
        "  sonar_forest1 = random_forest_algorithm(train_sonar1,sonar_features,sonar_label,\n",
        "                                       sonar_n_trees,sonar_boot,sonar_n_features,\n",
        "                                       best_sb1_depth)\n",
        "  # get train predictions\n",
        "  s1_train_predictions = get_random_forest_predictions(train_sonar1, sonar_forest1)\n",
        "  # get train score\n",
        "  s1_train_score = accuracy_score(train_sonar1[sonar_label],s1_train_predictions)\n",
        "  # append score\n",
        "  train_accuracies1.append(s1_train_score)\n",
        "  # get test predictions \n",
        "  s1_test_predictions = get_random_forest_predictions(test_sonar1, sonar_forest1)\n",
        "  # get test score\n",
        "  s1_test_score = accuracy_score(test_sonar1[sonar_label],s1_test_predictions)\n",
        "  # append score\n",
        "  test_accuracies1.append(s1_test_score)\n",
        "\n",
        "# caculate average\n",
        "avg_train_accuracy1 = sum(train_accuracies1)/len(train_accuracies1)\n",
        "abg_test_accuracy1 = sum(test_accuracies1)/len(test_accuracies1)\n",
        "print(\"Average Train Accuracy: \" + str(avg_train_accuracy1))\n",
        "print(\"Average Test Accuracy \" + str(abg_test_accuracy1))\n",
        "print(train_accuracies1)\n",
        "print(test_accuracies1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "OBnUMaqM9TIY"
      },
      "outputs": [],
      "source": [
        "###### BIN STRATEGY 2 (Median) ######\n",
        "\n",
        "## Data Preprocessing for Bin Strategy 2\n",
        "\n",
        "# initalize empty df\n",
        "sonar_bin2 = pd.DataFrame()\n",
        "\n",
        "# initilaize empty list for features\n",
        "sonar_features = []\n",
        "\n",
        "# add new labels to empty df based on their values relative to the column median\n",
        "for i in range(sonar.shape[1]-1): # minus 1 to not include target data frame\n",
        "  col_name = \"attribute_{}\".format(i+1)\n",
        "  # extract features\n",
        "  sonar_features.append(col_name)\n",
        "\n",
        "  sonar_bin2[col_name] = pd.qcut(sonar.iloc[:,i],\n",
        "                                      q = 2,\n",
        "                                      labels = [\"one\",\"two\"])\n",
        "  \n",
        "# add target column to new df and replace\n",
        "sonar_bin2['class'] = sonar['Class']\n",
        "sonar_bin2['class'] = sonar_bin2['class'].replace(['Rock','Mine'],[0,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "LOOv-873EHs3"
      },
      "outputs": [],
      "source": [
        "## Training and Testing Sonar Data Set with Bin Strategy 2 for ID3 Random Forest\n",
        "train_sonar2, test_sonar2 = split_data(sonar_bin2, 0.7)\n",
        "\n",
        "train_sonar2 = train_sonar2.dropna()\n",
        "test_sonar2 = test_sonar2.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dpiy47CDtNV",
        "outputId": "79f4250a-c388-4c4f-9137-a2cdf43e767a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth: 3 , \t train_accuracy:  0.8698630136986302 , \t test_accuracy: 0.7580645161290323\n",
            "max_depth: 4 , \t train_accuracy:  0.9041095890410958 , \t test_accuracy: 0.7096774193548387\n",
            "max_depth: 5 , \t train_accuracy:  0.952054794520548 , \t test_accuracy: 0.8225806451612904\n",
            "The best depth: 5\n"
          ]
        }
      ],
      "source": [
        "# Get hyper parameters \n",
        "best_sb2_depth = get_best_hyperparameters(train_sonar2,test_sonar2,sonar_features,sonar_label,\n",
        "                                                    sonar_n_trees, sonar_boot, sonar_n_features,[3,4,5])\n",
        "print(\"The best depth: \" + str(best_sb2_depth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ke-ucciF9to",
        "outputId": "e79a876f-e044-4914-d74d-ee84ba1bf2a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.941095890410959\n",
            "Average Test Accuracy 0.7258064516129032\n",
            "[0.9383561643835616, 0.9794520547945206, 0.9383561643835616, 0.9246575342465754, 0.9246575342465754]\n",
            "[0.7580645161290323, 0.7096774193548387, 0.7419354838709677, 0.7419354838709677, 0.6774193548387096]\n"
          ]
        }
      ],
      "source": [
        "# Get Average Accuracies for 5 tests for Bin Strategy 2\n",
        "\n",
        "# initialize empty lists\n",
        "train_accuracies2 = []\n",
        "test_accuracies2 = []\n",
        "# loop 5 times (to save time)\n",
        "for i in range(5):\n",
        "  # generate forest\n",
        "  sonar_forest2 = random_forest_algorithm(train_sonar2,sonar_features,sonar_label,\n",
        "                                       sonar_n_trees,sonar_boot,sonar_n_features,\n",
        "                                       best_sb2_depth)\n",
        "  # get train predictions\n",
        "  s2_train_predictions = get_random_forest_predictions(train_sonar2, sonar_forest2)\n",
        "  # get train score\n",
        "  s2_train_score = accuracy_score(train_sonar2[sonar_label],s2_train_predictions)\n",
        "  # append score\n",
        "  train_accuracies2.append(s2_train_score)\n",
        "  # get test predictions\n",
        "  s2_test_predictions = get_random_forest_predictions(test_sonar2, sonar_forest2)\n",
        "  # get test score\n",
        "  s2_test_score = accuracy_score(test_sonar2[sonar_label],s2_test_predictions)\n",
        "  # append score\n",
        "  test_accuracies2.append(s2_test_score)\n",
        "# calculate averages\n",
        "avg_train_accuracy2 = sum(train_accuracies2)/len(train_accuracies2)\n",
        "abg_test_accuracy2 = sum(test_accuracies2)/len(test_accuracies2)\n",
        "print(\"Average Train Accuracy: \" + str(avg_train_accuracy2))\n",
        "print(\"Average Test Accuracy \" + str(abg_test_accuracy2))\n",
        "print(train_accuracies2)\n",
        "print(test_accuracies2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95l50nH0_22Z",
        "outputId": "d2c1dec5-939f-4466-cd48-51f0d6f8cb3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy for sonar_bin1: 0.8551724137931035\n",
            "Test Accuracy for sonar_bin10.6774193548387096\n",
            "----------------------------------\n",
            "Train Accuracy for sonar_bin1: 0.9041095890410958\n",
            "Test Accuracy for sonar_bin10.7096774193548387\n"
          ]
        }
      ],
      "source": [
        "################## TESTING ID3 Algorithm ON SONAR DATA SET ##################\n",
        "# get best depth with cross validation\n",
        "bestDepth1, depthAccuracy_df1, testAccuracy1 = ID3_cross_validation(train_sonar1,10,sonar_features,sonar_label,[3,4,5])\n",
        "# generate tree with regular ID3 decision tree algorithm from homework\n",
        "bestTree1 = ID3_regular(train_sonar1,sonar_features,sonar_label,bestDepth1) \n",
        "# get train predcitions\n",
        "regTrain_predictions1 = ID3_decision_tree_all(bestTree1,train_sonar1)\n",
        "# get train score\n",
        "regTrain_score1 = accuracy_score(train_sonar1[sonar_label],regTrain_predictions1)\n",
        "# get test predictions\n",
        "regTest_predictions1 = ID3_decision_tree_all(bestTree1,test_sonar1)\n",
        "# get test score\n",
        "regTest_score1 = accuracy_score(test_sonar1[sonar_label],regTest_predictions1)\n",
        "\n",
        "print(\"Train Accuracy for sonar_bin1: \" + str(regTrain_score1))\n",
        "print(\"Test Accuracy for sonar_bin1\" + str(regTest_score1))\n",
        "print(\"----------------------------------\")\n",
        "\n",
        "# get best depth with cross validation\n",
        "bestDepth2, depthAccuracy_df2, testAccuracy2 = ID3_cross_validation(train_sonar2,10,sonar_features,sonar_label,[3,4,5,6])\n",
        "# generate tree with regular ID3 decision tree algorithm from homework\n",
        "bestTree2 = ID3_regular(train_sonar2,sonar_features,sonar_label,bestDepth2) \n",
        "# get train predcitions\n",
        "regTrain_predictions2 = ID3_decision_tree_all(bestTree2,train_sonar2)\n",
        "# get train score\n",
        "regTrain_score2 = accuracy_score(train_sonar2[sonar_label],regTrain_predictions2)\n",
        "# get test predictions\n",
        "regTest_predictions2 = ID3_decision_tree_all(bestTree2,test_sonar2)\n",
        "# get test score\n",
        "regTest_score2 = accuracy_score(test_sonar2[sonar_label],regTest_predictions2)\n",
        "\n",
        "print(\"Train Accuracy for sonar_bin1: \" + str(regTrain_score2))\n",
        "print(\"Test Accuracy for sonar_bin1\" + str(regTest_score2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "aae4EnT1TV17"
      },
      "outputs": [],
      "source": [
        "################## TESTING ID3 RANDOM FOREST ON BANKNOTES DATA SET ##################\n",
        "\n",
        "# Load Data and drop NAs (only 1 NA)\n",
        "banknotes = pd.read_csv(\"/content/data_banknote_authentication-1.csv\")\n",
        "banknotes = banknotes.dropna()\n",
        "\n",
        "\n",
        "###### BIN STRATEGY 1 (MEAN) ######\n",
        "\n",
        "## Data Preprocessing \n",
        "\n",
        "# get the mean of each column and turn to list\n",
        "col_mean2 = banknotes.mean(axis=0).tolist()\n",
        "col_mean2 = col_mean2[:-1]\n",
        "\n",
        "# initalize empty df\n",
        "bank_bin1 = pd.DataFrame()\n",
        "\n",
        "# bank notes features\n",
        "bank_features = ['variance','skewness','curtosis','entropy']\n",
        "\n",
        "# add new labels to empty df based on their values relative to the column mean\n",
        "for i in range(len(col_mean2)):\n",
        "  col_name = bank_features[i]\n",
        "  # add new columns with mutated values\n",
        "  bank_bin1[col_name] = pd.cut(banknotes.iloc[:,i],\n",
        "                                      bins = [-100,col_mean2[i],100],\n",
        "                                      labels = [\"lower\",\"higher\"])\n",
        "  \n",
        "# add target column to new df and replace\n",
        "bank_bin1['class'] = banknotes['class']\n",
        "\n",
        "## Training and Testing Banknotes Data Set with Bin Strategy 1 for ID3 Random Forest\n",
        "\n",
        "# initialize parameters\n",
        "bank_boot = int(bank_bin1.shape[0]*0.75) #added more training data due to over fitting\n",
        "bank_n_features = int(math.sqrt(len(bank_features)))\n",
        "bank_label = 'class'\n",
        "bank_trees = 50\n",
        "# split data into train and test\n",
        "train_bank1, test_bank1 = split_data(bank_bin1, 0.7) \n",
        "# omit na's just in case\n",
        "train_bank1 = train_bank1.dropna()\n",
        "test_bank1 = test_bank1.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKYemS6IWQfF",
        "outputId": "20e8f82d-3372-40a1-aba0-4e8bf28251d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth: 2 , \t train_accuracy:  0.846875 , \t test_accuracy: 0.8422330097087378\n",
            "max_depth: 3 , \t train_accuracy:  0.8010416666666667 , \t test_accuracy: 0.779126213592233\n",
            "max_depth: 4 , \t train_accuracy:  0.8010416666666667 , \t test_accuracy: 0.779126213592233\n",
            "The best depth: 2\n"
          ]
        }
      ],
      "source": [
        "# Get hyperparameters \n",
        "best_bn1_depth= get_best_hyperparameters(train_bank1,test_bank1,bank_features,bank_label,\n",
        "                                                    bank_trees, bank_boot, bank_n_features,[2,3,4]) \n",
        "print(\"The best depth: \" + str(best_bn1_depth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gibhyQ8Ke_tt",
        "outputId": "f0573ce4-ce5b-429c-b14f-10dd514ee06e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.8188541666666665\n",
            "Average Test Accuracy 0.8036407766990292\n",
            "[0.7958333333333333, 0.846875, 0.846875, 0.8010416666666667, 0.846875, 0.8010416666666667, 0.8010416666666667, 0.8010416666666667, 0.8010416666666667, 0.846875]\n",
            "[0.7718446601941747, 0.8422330097087378, 0.8422330097087378, 0.779126213592233, 0.8422330097087378, 0.779126213592233, 0.779126213592233, 0.779126213592233, 0.779126213592233, 0.8422330097087378]\n"
          ]
        }
      ],
      "source": [
        "# Get Average Accuracies for 10 tests for Bin Strategy 1\n",
        "# initialize empty lists\n",
        "trainAccuracies1 = []\n",
        "testAccuracies1 = []\n",
        "# loop 10 times\n",
        "for i in range(10):\n",
        "  # generate random forest\n",
        "  bank_forest1 = random_forest_algorithm(train_bank1,bank_features,bank_label,\n",
        "                                       bank_trees,bank_boot,bank_n_features,\n",
        "                                       best_bn1_depth)\n",
        "  # get train predictions\n",
        "  b1_train_predictions = get_random_forest_predictions(train_bank1, bank_forest1)\n",
        "  # get train score\n",
        "  b1_train_score = accuracy_score(train_bank1[bank_label],b1_train_predictions)\n",
        "  # append score\n",
        "  trainAccuracies1.append(b1_train_score)\n",
        "  # get test predictions\n",
        "  b1_test_predictions = get_random_forest_predictions(test_bank1, bank_forest1)\n",
        "  # get test score\n",
        "  b1_test_score = accuracy_score(test_bank1[bank_label],b1_test_predictions)\n",
        "  # append score\n",
        "  testAccuracies1.append(b1_test_score)\n",
        "# calculate averages\n",
        "avg_trainAccuracy1 = sum(trainAccuracies1)/len(trainAccuracies1)\n",
        "avg_testAccuracy1 = sum(testAccuracies1)/len(testAccuracies1)\n",
        "print(\"Average Train Accuracy: \" + str(avg_trainAccuracy1))\n",
        "print(\"Average Test Accuracy \" + str(avg_testAccuracy1))\n",
        "print(trainAccuracies1)\n",
        "print(testAccuracies1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bO936TLwgrSt"
      },
      "outputs": [],
      "source": [
        "###### BIN STRATEGY 2 (Median) ######\n",
        "\n",
        "## Data Preprocessing \n",
        "\n",
        "# initalize empty df\n",
        "bank_bin2 = pd.DataFrame()\n",
        "\n",
        "# add new labels to empty df based on their values relative to the column mean\n",
        "for i in range(banknotes.shape[1]-1):\n",
        "  col_name = bank_features[i]\n",
        "  # add new columns with mutated values\n",
        "  bank_bin2[col_name] = pd.qcut(banknotes.iloc[:,i],\n",
        "                                      q = 2,\n",
        "                                      labels = [\"lower\",\"higher\"])\n",
        "  \n",
        "# add target column to new df and replace\n",
        "bank_bin2['class'] = banknotes['class']\n",
        "\n",
        "## Training and Testing Banknotes Data Set with Bin Strategy 1 for ID3 Random Forest\n",
        "\n",
        "# splits data into train and test\n",
        "train_bank2, test_bank2 = split_data(bank_bin2, 0.7)\n",
        "# omit na's just in case\n",
        "train_bank2 = train_bank1.dropna()\n",
        "test_bank2 = test_bank1.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTiw1OMZrHyc",
        "outputId": "b22643c3-0902-42a6-cca1-69221e64e5d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_depth: 2 , \t train_accuracy:  0.8010416666666667 , \t test_accuracy: 0.779126213592233\n",
            "max_depth: 3 , \t train_accuracy:  0.846875 , \t test_accuracy: 0.8422330097087378\n",
            "max_depth: 4 , \t train_accuracy:  0.846875 , \t test_accuracy: 0.8422330097087378\n",
            "The best depth: 4\n"
          ]
        }
      ],
      "source": [
        "# Get hyper parameters \n",
        "best_bn2_depth = get_best_hyperparameters(train_bank2,test_bank2,bank_features,bank_label,\n",
        "                                                    bank_trees, bank_boot, bank_n_features,[2,3,4])\n",
        "print(\"The best depth: \" + str(best_bn2_depth))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdYHdyEsvhoc",
        "outputId": "9af1531d-aa78-4049-fc70-2508a3b9f3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Train Accuracy: 0.846875\n",
            "Average Test Accuracy 0.81626213592233\n",
            "[0.846875, 0.846875, 0.846875, 0.846875, 0.846875, 0.846875, 0.846875, 0.846875, 0.846875, 0.846875]\n",
            "[0.8422330097087378, 0.8422330097087378, 0.779126213592233, 0.8422330097087378, 0.8422330097087378, 0.779126213592233, 0.8422330097087378, 0.7718446601941747, 0.779126213592233, 0.8422330097087378]\n"
          ]
        }
      ],
      "source": [
        "# Get Average Accuracies for 10 tests for Bin Strategy 2\n",
        "trainAccuracies2 = []\n",
        "testAccuracies2 = []\n",
        "for i in range(10):\n",
        "  bank_forest2 = random_forest_algorithm(train_bank2,bank_features,bank_label,\n",
        "                                       bank_trees,bank_boot,bank_n_features,\n",
        "                                       best_bn2_depth)\n",
        "  b2_train_predictions = get_random_forest_predictions(train_bank2, bank_forest1)\n",
        "  b2_train_score = accuracy_score(train_bank2[bank_label],b2_train_predictions)\n",
        "  trainAccuracies2.append(b2_train_score)\n",
        "  b2_test_predictions = get_random_forest_predictions(test_bank2, bank_forest2)\n",
        "  b2_test_score = accuracy_score(test_bank2[bank_label],b2_test_predictions)\n",
        "  testAccuracies2.append(b2_test_score)\n",
        "\n",
        "avg_trainAccuracy2 = sum(trainAccuracies2)/len(trainAccuracies2)\n",
        "avg_testAccuracy2 = sum(testAccuracies2)/len(testAccuracies2)\n",
        "print(\"Average Train Accuracy: \" + str(avg_trainAccuracy2))\n",
        "print(\"Average Test Accuracy \" + str(avg_testAccuracy2))\n",
        "print(trainAccuracies2)\n",
        "print(testAccuracies2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smrle8JY0FbG",
        "outputId": "66086105-4f82-4456-9247-f5d9d06e7e74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy for bank_bin1: 0.846875\n",
            "Test Accuracy for bank_bin10.8422330097087378\n",
            "----------------------------------\n",
            "Train Accuracy for bank_bin2: 0.846875\n",
            "Test Accuracy for bank_bin20.8422330097087378\n"
          ]
        }
      ],
      "source": [
        "################## TESTING ID3 Algorithm ON BANKNOTES DATA SET ##################\n",
        "# get best depth with cross validation from class\n",
        "best_depth1, depth_accuracy_df1, test_accuracy1 = ID3_cross_validation(train_bank1,10,bank_features,bank_label,[1,2,3,4,5,6,7,8,9,10])\n",
        "# get tree with regular ID3 algorithm from homework\n",
        "best_tree1 = ID3_regular(train_bank1,bank_features,bank_label,best_depth1) \n",
        "# get train predictions\n",
        "reg_train_predictions1 = ID3_decision_tree_all(best_tree1,train_bank1)\n",
        "#get train score\n",
        "reg_train_score1 = accuracy_score(train_bank1[bank_label],reg_train_predictions1)\n",
        "# get test predicitions\n",
        "reg_test_predictions1 = ID3_decision_tree_all(best_tree1,test_bank1)\n",
        "#get test score\n",
        "reg_test_score1 = accuracy_score(test_bank1[bank_label],reg_test_predictions1)\n",
        "\n",
        "print(\"Train Accuracy for bank_bin1: \" + str(reg_train_score1))\n",
        "print(\"Test Accuracy for bank_bin1\" + str(reg_test_score1))\n",
        "print(\"----------------------------------\")\n",
        "# get best depth with cross validation from class\n",
        "best_depth2, depth_accuracy_df2, test_accuracy2 = ID3_cross_validation(train_bank2,10,bank_features,bank_label,[1,2,3,4,5,6,7,8,9,10])\n",
        "# get tree with regular ID3 algorithm from homework\n",
        "best_tree2 = ID3_regular(train_bank2,bank_features,bank_label,best_depth2) \n",
        "# get train predictions\n",
        "reg_train_predictions2 = ID3_decision_tree_all(best_tree2,train_bank2)\n",
        "#get train score\n",
        "reg_train_score2 = accuracy_score(train_bank2[bank_label],reg_train_predictions2)\n",
        "# get test predicitions\n",
        "reg_test_predictions2 = ID3_decision_tree_all(best_tree2,test_bank2)\n",
        "#get test score\n",
        "reg_test_score2 = accuracy_score(test_bank2[bank_label],reg_test_predictions2)\n",
        "\n",
        "print(\"Train Accuracy for bank_bin2: \" + str(reg_train_score2))\n",
        "print(\"Test Accuracy for bank_bin2\" + str(reg_test_score2))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "COMP343 Final Project ID3 Random Forest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}